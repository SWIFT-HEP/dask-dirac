{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Graph\n",
    "Notebook to manipulate a dask graph at the client stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Edit submit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client()\n",
    "\n",
    "original_submit = client.submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_submit(*args, **kwargs):\n",
    "    print(\"Entered Submit\")\n",
    "    print(\"Party if it works\")\n",
    "    return original_submit(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.submit = modified_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered Submit\n",
      "Party if it works\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>Future: neg</strong>\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> status: </span>\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-error-color0, black)\">pending</span>,\n",
       "\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> type:</span> NoneType,\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> key:</span> neg-6509467a9c66ce2389f8f71ccfe663b3"
      ],
      "text/plain": [
       "<Future: pending, key: neg-6509467a9c66ce2389f8f71ccfe663b3>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def neg(x):\n",
    "    return -x\n",
    "\n",
    "\n",
    "client.submit(neg, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 19:37:55,387 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:16903' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'neg-6509467a9c66ce2389f8f71ccfe663b3'} (stimulus_id='handle-worker-cleanup-1718822275.387008')\n"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Edit collections_to_dsk method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Save original collections_to_dsk method\n",
    "original_collections_to_dsk = client.collections_to_dsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections_to_dsk method has been replaced\n"
     ]
    }
   ],
   "source": [
    "def modified_collections_to_dsk(collections, *args, **kwargs):\n",
    "    c = datetime.now()\n",
    "    print(\"Inside myFunc. Time = \" + c.strftime(\"%H:%M:%S\"))\n",
    "    print(\"Dask Graph Tasks\")\n",
    "    for collection in collections:\n",
    "        print(f\"Task Name: {collection.name}\")\n",
    "    print(\"-----------------\\n\")\n",
    "\n",
    "    return original_collections_to_dsk(collections, *args, **kwargs)\n",
    "\n",
    "\n",
    "client.collections_to_dsk = modified_collections_to_dsk\n",
    "print(\"collections_to_dsk method has been replaced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.ones((1000, 1000), chunks=(100, 100))\n",
    "y = x + 1\n",
    "z = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside myFunc. Time = 19:38:18\n",
      "Dask Graph Tasks\n",
      "Task Name: mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = client.compute(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 19:38:31,549 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:21019' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'finalize-3daf4f11543bcdbd0d181b8d6c929af3'} (stimulus_id='handle-worker-cleanup-1718822311.5493789')\n"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Replace a Task Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1: Keep client running\n",
    "This will cause an issue as the client still has memory of the task and hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "from datetime import datetime\n",
    "\n",
    "client = Client()\n",
    "\n",
    "original_collections_to_dsk = client.collections_to_dsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_functions = []\n",
    "\n",
    "\n",
    "def add_to_three(number):\n",
    "    return number + 3\n",
    "\n",
    "\n",
    "def modified_collections_to_dsk(collections, *args, **kwargs):\n",
    "    c = datetime.now()\n",
    "    print(\"Inside myFunc. Time = \" + c.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    print(\"Functions already processed\")\n",
    "    global existing_functions\n",
    "    for funcs in existing_functions:\n",
    "        print(f\"{funcs}\")\n",
    "\n",
    "    print(\"Dask Graph Tasks\")\n",
    "    for collection in collections:\n",
    "        print(f\"Task Name: {collection.name}\")\n",
    "        print(\"Dask Graph\")\n",
    "        print(collection.dask)\n",
    "        if collection.name not in existing_functions:\n",
    "            print(f\"Adding {collection.name} to existing_functions\")\n",
    "            existing_functions.append(collection.name)\n",
    "        else:\n",
    "            print(f\"{collection.name} has already been processed\")\n",
    "            # TODO: Continue playing around with the dask graph so return is not a string\n",
    "            # TODO: Use collection hash rather than collection name\n",
    "            layers = {\n",
    "                f\"add_to_three-{(collection.name)}\": {\n",
    "                    (f\"add_to_three-{(collection.name)}\", 0): (add_to_three, 20)\n",
    "                }\n",
    "            }\n",
    "            dependencies = {f\"add_to_three-{(collection.name)}\": set()}\n",
    "            graph = HighLevelGraph(layers, dependencies)\n",
    "            collection.dask = graph\n",
    "            print(\"New Dask Graph\")\n",
    "            print(collection.dask)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    return original_collections_to_dsk(collections, *args, **kwargs)\n",
    "\n",
    "\n",
    "client.collections_to_dsk = modified_collections_to_dsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.ones((1000, 1000), chunks=(100, 100))\n",
    "y = x + 1\n",
    "z = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside myFunc. Time = 19:40:19\n",
      "Functions already processed\n",
      "Dask Graph Tasks\n",
      "Task Name: mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "Dask Graph\n",
      "HighLevelGraph with 7 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7f95fa07f3a0>\n",
      " 0. ones_like-c53a97836143a00162e3470fef213e1e\n",
      " 1. add-098526cf93d1ac0d3a8744f59d0814fb\n",
      " 2. mean_chunk-4e5168d025885dfbdfd76df3f8083cda\n",
      " 3. mean_combine-partial-0832826c732652f3243668d740a0d5f3\n",
      " 4. mean_combine-partial-f33215f12dcb169d48ded3405ae5b4a2\n",
      " 5. mean_combine-partial-f380b3499d120118c907e62d760350cc\n",
      " 6. mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "\n",
      "Adding mean_agg-aggregate-c50263a0ad47c330577b3058195ad928 to existing_functions\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "result = client.compute(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>Future: finalize</strong>\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> status: </span>\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-error-color0, black)\">finished</span>,\n",
       "\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> type:</span> numpy.float64,\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> key:</span> finalize-3daf4f11543bcdbd0d181b8d6c929af3"
      ],
      "text/plain": [
       "<Future: finished, type: numpy.float64, key: finalize-3daf4f11543bcdbd0d181b8d6c929af3>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside myFunc. Time = 19:40:23\n",
      "Functions already processed\n",
      "mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "Dask Graph Tasks\n",
      "Task Name: mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "Dask Graph\n",
      "HighLevelGraph with 7 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7f95fa07f3a0>\n",
      " 0. ones_like-c53a97836143a00162e3470fef213e1e\n",
      " 1. add-098526cf93d1ac0d3a8744f59d0814fb\n",
      " 2. mean_chunk-4e5168d025885dfbdfd76df3f8083cda\n",
      " 3. mean_combine-partial-0832826c732652f3243668d740a0d5f3\n",
      " 4. mean_combine-partial-f33215f12dcb169d48ded3405ae5b4a2\n",
      " 5. mean_combine-partial-f380b3499d120118c907e62d760350cc\n",
      " 6. mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "\n",
      "mean_agg-aggregate-c50263a0ad47c330577b3058195ad928 has already been processed\n",
      "New Dask Graph\n",
      "HighLevelGraph with 1 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7f95f8773c70>\n",
      " 0. add_to_three-mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 19:40:23,945 - distributed.scheduler - WARNING - Detected different `run_spec` for key 'finalize-3daf4f11543bcdbd0d181b8d6c929af3' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \n",
      "Debugging information\n",
      "---------------------\n",
      "old task state: memory\n",
      "old run_spec: (<function finalize at 0x7f96080ec280>, ([('mean_agg-aggregate-c50263a0ad47c330577b3058195ad928',)],), {})\n",
      "new run_spec: (<function finalize at 0x7f96080ec280>, ([('mean_agg-aggregate-c50263a0ad47c330577b3058195ad928',)],), {})\n",
      "old token: ('tuple', [('ae65af090ad79ae2e434bc4c7eb750244a1a40bb', []), ('tuple', [('list', [('tuple', ['mean_agg-aggregate-c50263a0ad47c330577b3058195ad928'])])]), ('dict', [])])\n",
      "new token: ('tuple', [('ae65af090ad79ae2e434bc4c7eb750244a1a40bb', []), ('tuple', [('list', [('tuple', ['mean_agg-aggregate-c50263a0ad47c330577b3058195ad928'])])]), ('dict', [])])\n",
      "old dependencies: {('mean_agg-aggregate-c50263a0ad47c330577b3058195ad928',)}\n",
      "new dependencies: set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute a second time\n",
    "result = client.compute(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 19:40:46,904 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:13361' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'finalize-3daf4f11543bcdbd0d181b8d6c929af3'} (stimulus_id='handle-worker-cleanup-1718822446.9042947')\n"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.2: Edit HighLevelGraph in a new cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "original_collections_to_dsk = client.collections_to_dsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_functions = [\"mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\"]\n",
    "\n",
    "\n",
    "def modified_collections_to_dsk(collections, *args, **kwargs):\n",
    "    c = datetime.now()\n",
    "    print(\"Inside myFunc. Time = \" + c.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    print(\"Functions already processed\")\n",
    "    global existing_functions\n",
    "    for funcs in existing_functions:\n",
    "        print(f\"{funcs}\")\n",
    "\n",
    "    print(\"Dask Graph Tasks\")\n",
    "    for collection in collections:\n",
    "        print(f\"Task Name: {collection.name}\")\n",
    "        print(\"Dask Graph\")\n",
    "        print(collection.dask)\n",
    "        if collection.name not in existing_functions:\n",
    "            print(f\"Adding {collection.name} to existing_functions\")\n",
    "            existing_functions.append(collection.name)\n",
    "        else:\n",
    "            print(f\"{collection.name} has already been processed\")\n",
    "            # TODO: Continue playing around with the dask graph so return is not a string\n",
    "            # TODO: Use collection hash rather than collection name\n",
    "            layers = {\n",
    "                f\"add_to_three-{(collection.name)}\": {\n",
    "                    (f\"add_to_three-{(collection.name)}\", 0): (add_to_three, 20)\n",
    "                }\n",
    "            }\n",
    "            dependencies = {f\"add_to_three-{(collection.name)}\": set()}\n",
    "            graph = HighLevelGraph(layers, dependencies)\n",
    "            collection.dask = graph\n",
    "            # layers = {f\"add_to_thre11e-{(collection.name)}\": {(f\"add_to_three-{(collection.name)}\", 0): (add_to_three, 20)}}\n",
    "            # layers = {f\"add_to_thre11e-{(collection.name)}\": (add_to_three, 20)}\n",
    "            # dependencies = {f\"add_to_three-{(collection.name)}\": set()}\n",
    "            # graph = HighLevelGraph(layers, dependencies)\n",
    "            # collection.dask = graph\n",
    "            print(\"New Dask Graph\")\n",
    "            print(collection.dask)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    return original_collections_to_dsk(collections, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.collections_to_dsk = modified_collections_to_dsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_three(number):\n",
    "    return number + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside myFunc. Time = 19:43:16\n",
      "Functions already processed\n",
      "mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "Dask Graph Tasks\n",
      "Task Name: mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "Dask Graph\n",
      "HighLevelGraph with 7 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7f9608d4ba90>\n",
      " 0. ones_like-c53a97836143a00162e3470fef213e1e\n",
      " 1. add-098526cf93d1ac0d3a8744f59d0814fb\n",
      " 2. mean_chunk-4e5168d025885dfbdfd76df3f8083cda\n",
      " 3. mean_combine-partial-0832826c732652f3243668d740a0d5f3\n",
      " 4. mean_combine-partial-f33215f12dcb169d48ded3405ae5b4a2\n",
      " 5. mean_combine-partial-f380b3499d120118c907e62d760350cc\n",
      " 6. mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "\n",
      "mean_agg-aggregate-c50263a0ad47c330577b3058195ad928 has already been processed\n",
      "New Dask Graph\n",
      "HighLevelGraph with 1 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7f95fa07fa00>\n",
      " 0. add_to_three-mean_agg-aggregate-c50263a0ad47c330577b3058195ad928\n",
      "\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "x = da.ones((1000, 1000), chunks=(100, 100))\n",
    "y = x + 1\n",
    "z = y.mean()\n",
    "result = client.compute(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>Future: finalize</strong>\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> status: </span>\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-error-color0, black)\">finished</span>,\n",
       "\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> type:</span> str,\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> key:</span> finalize-3daf4f11543bcdbd0d181b8d6c929af3"
      ],
      "text/plain": [
       "<Future: finished, type: str, key: finalize-3daf4f11543bcdbd0d181b8d6c929af3>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean_agg-aggregate-c50263a0ad47c330577b3058195ad928'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.3: Try with blockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/swift-hep/envs/dask_dirac/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 16985 instead\n",
      "  warnings.warn(\n",
      "2024-06-24 22:40:59,285 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:16327' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'finalize-99beb22034172632fdd161427fa0c2cc'} (stimulus_id='handle-worker-cleanup-1719265259.285501')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "[20 35 54]\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "from datetime import datetime\n",
    "from dask.array.core import Array\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define some simple functions to use in the graph\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "\n",
    "\n",
    "# Create a Dask array from the HighLevelGraph\n",
    "# Note: We need to specify the shape and chunks of the resulting array\n",
    "dask_array = da.blockwise(\n",
    "    lambda x, y: multiply(add(x, y), y),\n",
    "    \"i\",\n",
    "    np.array([1, 2, 3]),\n",
    "    \"i\",\n",
    "    np.array([4, 5, 6]),\n",
    "    \"i\",\n",
    "    dtype=np.int64,\n",
    ")\n",
    "\n",
    "# Use client.compute to compute the result\n",
    "future = client.compute(dask_array)\n",
    "result = future.result()\n",
    "\n",
    "client.shutdown()\n",
    "\n",
    "print(\"------------\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/swift-hep/envs/dask_dirac/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 25436 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside myFunc. Time = 22:41:44\n",
      "Functions already processed\n",
      "Dask Graph Tasks\n",
      "Task Name: lambda-bd96b5ee7bd8b34908deaa71441f1d4a\n",
      "Dask Graph\n",
      "HighLevelGraph with 3 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7fae2833ff10>\n",
      " 0. array-93899268a02d75f85755ccb59b755ad1\n",
      " 1. array-08b8023b2db8bebf73813d166e3f6b63\n",
      " 2. lambda-bd96b5ee7bd8b34908deaa71441f1d4a\n",
      "\n",
      "-----------------\n",
      "Edditing Dask Graph\n",
      "New Dask Graph\n",
      "HighLevelGraph with 3 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7fae481a7190>\n",
      " 0. array-71a7394e5f14de7dc72bb0b1b251ce84\n",
      " 1. array-e3622cdb87acdb3eed57eec6c2ef9fb7\n",
      " 2. lambda-19e9145bca12a9b712321a40e73e64bd\n",
      "\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 22:41:45,478 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:21393' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'finalize-a78071afc5b678a3bf5936b8a138ce3f'} (stimulus_id='handle-worker-cleanup-1719265305.478143')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "[104 135 170]\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "from datetime import datetime\n",
    "from dask.array.core import Array\n",
    "\n",
    "\n",
    "def modified_collections_to_dsk(collections, *args, **kwargs):\n",
    "    c = datetime.now()\n",
    "    print(\"Inside myFunc. Time = \" + c.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    print(\"Functions already processed\")\n",
    "\n",
    "    print(\"Dask Graph Tasks\")\n",
    "    for collection in collections:\n",
    "        print(f\"Task Name: {collection.name}\")\n",
    "        print(\"Dask Graph\")\n",
    "        print(collection.dask)\n",
    "        print(\"-----------------\")\n",
    "        print(\"Edditing Dask Graph\")\n",
    "        dask_array = da.blockwise(\n",
    "            lambda x, y: multiply(add(x, y), y),\n",
    "            \"i\",\n",
    "            np.array([5, 6, 7]),\n",
    "            \"i\",\n",
    "            np.array([8, 9, 10]),\n",
    "            \"i\",\n",
    "            dtype=np.int64,\n",
    "        )\n",
    "        collection.dask = dask_array.dask\n",
    "        collection.__dask_keys__ = dask_array.__dask_keys__\n",
    "\n",
    "        print(\"New Dask Graph\")\n",
    "        print(collection.dask)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    return original_collections_to_dsk(collections, *args, **kwargs)\n",
    "\n",
    "\n",
    "client = Client()\n",
    "\n",
    "original_collections_to_dsk = client.collections_to_dsk\n",
    "\n",
    "client.collections_to_dsk = modified_collections_to_dsk\n",
    "\n",
    "# Define some simple functions to use in the graph\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "\n",
    "\n",
    "# Create a Dask array from the HighLevelGraph\n",
    "# Note: We need to specify the shape and chunks of the resulting array\n",
    "dask_array = da.blockwise(\n",
    "    lambda x, y: multiply(add(x, y), y),\n",
    "    \"i\",\n",
    "    np.array([1, 2, 3]),\n",
    "    \"i\",\n",
    "    np.array([4, 5, 6]),\n",
    "    \"i\",\n",
    "    dtype=np.int64,\n",
    ")\n",
    "\n",
    "# Use client.compute to compute the result\n",
    "future = client.compute(dask_array)\n",
    "result = future.result()\n",
    "\n",
    "client.shutdown()\n",
    "\n",
    "print(\"------------\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.4: ones and blockwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a HighLevelGraph containing two tasks is submitted to the client.\n",
    "Prior to being sent to the scheduler, the HighLevelGraph is replaced with a HighLevelGraph defined by blockwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside myFunc. Time = 16:05:15\n",
      "Functions already processed\n",
      "Dask Graph Tasks\n",
      "Task Name: add-098526cf93d1ac0d3a8744f59d0814fb\n",
      "Dask Graph\n",
      "HighLevelGraph with 2 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7efc31c09210>\n",
      " 0. ones_like-c53a97836143a00162e3470fef213e1e\n",
      " 1. add-098526cf93d1ac0d3a8744f59d0814fb\n",
      "\n",
      "-----------------\n",
      "Edditing Dask Graph\n",
      "New Dask Graph\n",
      "HighLevelGraph with 3 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7efc31c09810>\n",
      " 0. array-71a7394e5f14de7dc72bb0b1b251ce84\n",
      " 1. array-e3622cdb87acdb3eed57eec6c2ef9fb7\n",
      " 2. lambda-75f9fecc0d97ebf395a496c2575f03d0\n",
      "\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 16:05:15,426 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:13193' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'finalize-c0d5d498f2169d9602849de30cd6a0a0'} (stimulus_id='handle-worker-cleanup-1719327915.4264529')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "[104 135 170]\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "from datetime import datetime\n",
    "from dask.array.core import Array\n",
    "\n",
    "\n",
    "def modified_collections_to_dsk(collections, *args, **kwargs):\n",
    "    c = datetime.now()\n",
    "    print(\"Inside myFunc. Time = \" + c.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    print(\"Functions already processed\")\n",
    "\n",
    "    print(\"Dask Graph Tasks\")\n",
    "    for collection in collections:\n",
    "        print(f\"Task Name: {collection.name}\")\n",
    "        print(\"Dask Graph\")\n",
    "        print(collection.dask)\n",
    "        print(\"-----------------\")\n",
    "        print(\"Edditing Dask Graph\")\n",
    "        dask_array = da.blockwise(\n",
    "            lambda x, y: multiply(add(x, y), y),\n",
    "            \"i\",\n",
    "            np.array([5, 6, 7]),\n",
    "            \"i\",\n",
    "            np.array([8, 9, 10]),\n",
    "            \"i\",\n",
    "            dtype=np.int64,\n",
    "        )\n",
    "        collection.dask = dask_array.dask\n",
    "        collection.__dask_keys__ = dask_array.__dask_keys__\n",
    "\n",
    "        print(\"New Dask Graph\")\n",
    "        print(collection.dask)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    return original_collections_to_dsk(collections, *args, **kwargs)\n",
    "\n",
    "\n",
    "client = Client()\n",
    "\n",
    "original_collections_to_dsk = client.collections_to_dsk\n",
    "\n",
    "client.collections_to_dsk = modified_collections_to_dsk\n",
    "\n",
    "# Define some simple functions to use in the graph\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "\n",
    "\n",
    "# Create a Dask array from the HighLevelGraph\n",
    "# Note: We need to specify the shape and chunks of the resulting array\n",
    "dask_array = da.ones((1000, 1000), chunks=(100,100)) + 1\n",
    "\n",
    "\n",
    "# Use client.compute to compute the result\n",
    "future = client.compute(dask_array)\n",
    "result = future.result()\n",
    "\n",
    "client.shutdown()\n",
    "\n",
    "print(\"------------\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.1: Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a hash is created using hashlib. \n",
    "dask.tokenize is not used as it is not repeatable in different environments.\n",
    "\n",
    "Expected hash is `ec2f2283b6f02591655b83abe2be321d08b8b54983b0d9083da5eda81e837d490c838fdb882d8891161f832d30ff1cd8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HighLevelGraph with 2 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7fd920bf3970>\n",
      " 0. ones_like-c53a97836143a00162e3470fef213e1e\n",
      " 1. add-34435cc4a153f02b208c5e30ee811e3d\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import hashlib\n",
    "\n",
    "dask_array = da.ones((1000, 1000), chunks=(100,100)) + 20231\n",
    "graph = dask_array.__dask_graph__()\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: ones_like-c53a97836143a00162e3470fef213e1e\n",
      "short_layer_name: ones_like\n",
      "len: 100\n",
      "args: ((100, 100),)\n",
      "args: (100, 100)\n",
      "function: subgraph_callable-f646aaf1ae08e118dd013a3d16eb54ec\n",
      "----------------------\n",
      "Layer: add-34435cc4a153f02b208c5e30ee811e3d\n",
      "short_layer_name: add\n",
      "len: 100\n",
      "args: (('ones_like-c53a97836143a00162e3470fef213e1e', 9, 9), 20231)\n",
      "args: 20231\n",
      "function: subgraph_callable-67aaf28825d396a0abf7512dfd79c838\n",
      "----------------------\n",
      "total graph description:\n",
      "[{'function:': 'ones_like', 'layer_length': 100, 'layer_args': (100, 100)}, {'function:': 'add', 'layer_length': 100, 'layer_args': 20231}]\n"
     ]
    }
   ],
   "source": [
    "total_graph_description = []\n",
    "for i, (layer_name, layer) in enumerate(graph.layers.items()):\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    short_layer_name = layer_name[:layer_name.rfind('-')]\n",
    "    print(f\"short_layer_name: {short_layer_name}\")\n",
    "    print(f\"len: {len(layer.items())}\")\n",
    "    # Lazy looping to get the args of one layer. \n",
    "    # Possible extention would get all hash of each layer\n",
    "    for task_key, task in layer.items():\n",
    "        function = task[0]\n",
    "        args = task[1:]\n",
    "    print(f\"args: {args}\")\n",
    "    if i == 0:\n",
    "        print(f'args: {args[0]}')\n",
    "        layer_args = args[0]\n",
    "        \n",
    "    else:\n",
    "        print(f'args: {args[1]}')\n",
    "        layer_args = args[1]\n",
    "    print(f'function: {function}')\n",
    "    layer_description = {\"function:\": short_layer_name,\n",
    "                         \"layer_length\": len(layer.items()),\n",
    "                         \"layer_args\": layer_args}\n",
    "    total_graph_description.append(layer_description)\n",
    "    print('----------------------')\n",
    "\n",
    "print('total graph description:')\n",
    "print(total_graph_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ec2f2283b6f02591655b83abe2be321d08b8b54983b0d9083da5eda81e837d490c838fdb882d8891161f832d30ff1cd8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashlib.sha3_384(str(total_graph_description).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ec2f2283b6f02591655b83abe2be321d08b8b54983b0d9083da5eda81e837d490c838fdb882d8891161f832d30ff1cd8`\n",
    "\n",
    "is the expected result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20232., 20232., 20232., ..., 20232., 20232., 20232.],\n",
       "       [20232., 20232., 20232., ..., 20232., 20232., 20232.],\n",
       "       [20232., 20232., 20232., ..., 20232., 20232., 20232.],\n",
       "       ...,\n",
       "       [20232., 20232., 20232., ..., 20232., 20232., 20232.],\n",
       "       [20232., 20232., 20232., ..., 20232., 20232., 20232.],\n",
       "       [20232., 20232., 20232., ..., 20232., 20232., 20232.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_array.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.2: Move the hash determination into collections_to_dsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from dask.distributed import Client\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_collections_to_dsk(collections, *args, **kwargs):\n",
    "    c = datetime.now()\n",
    "    print(\"Inside myFunc. Time = \" + c.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    print(\"Functions already processed\")\n",
    "\n",
    "    print(\"Dask Graph Tasks\")\n",
    "    for collection in collections:\n",
    "        graph = collection.dask\n",
    "\n",
    "        print(f\"Task Name: {collection.name}\")\n",
    "        print(\"Dask Graph\")\n",
    "        print(graph)\n",
    "        total_graph_description = []\n",
    "        for i, (layer_name, layer) in enumerate(graph.layers.items()):\n",
    "            short_layer_name = layer_name[:layer_name.rfind('-')]\n",
    "            task_key, task = next(iter(layer.items()))\n",
    "            function = task[0]\n",
    "            task_args = task[1:]\n",
    "            if i == 0:\n",
    "                layer_args = task_args[0]\n",
    "            else:\n",
    "                layer_args = task_args[1]\n",
    "    \n",
    "            layer_description = {\"function:\": short_layer_name,\n",
    "                                 \"layer_length\": len(layer.items()),\n",
    "                                 \"layer_args\": layer_args}\n",
    "            total_graph_description.append(layer_description)\n",
    "        print('total_graph_description:')\n",
    "        pprint(total_graph_description)\n",
    "        graph_hash = hashlib.sha3_384(str(total_graph_description).encode()).hexdigest()\n",
    "        print(f'hash: \\033[93m {graph_hash} \\033[0m')\n",
    "\n",
    "    print('----------------------')\n",
    "\n",
    "    return original_collections_to_dsk(collections, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside myFunc. Time = 20:57:57\n",
      "Functions already processed\n",
      "Dask Graph Tasks\n",
      "Task Name: add-34435cc4a153f02b208c5e30ee811e3d\n",
      "Dask Graph\n",
      "HighLevelGraph with 2 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7f6bf7ebb9d0>\n",
      " 0. ones_like-c53a97836143a00162e3470fef213e1e\n",
      " 1. add-34435cc4a153f02b208c5e30ee811e3d\n",
      "\n",
      "total_graph_description:\n",
      "[{'function:': 'ones_like', 'layer_args': (100, 100), 'layer_length': 100},\n",
      " {'function:': 'add', 'layer_args': 20231, 'layer_length': 100}]\n",
      "hash: \u001b[93m ec2f2283b6f02591655b83abe2be321d08b8b54983b0d9083da5eda81e837d490c838fdb882d8891161f832d30ff1cd8 \u001b[0m\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 20:57:58,396 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:5373' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'finalize-236d3b698353fe74f1e05c293988becb'} (stimulus_id='handle-worker-cleanup-1719950278.395871')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "[[20232. 20232. 20232. ... 20232. 20232. 20232.]\n",
      " [20232. 20232. 20232. ... 20232. 20232. 20232.]\n",
      " [20232. 20232. 20232. ... 20232. 20232. 20232.]\n",
      " ...\n",
      " [20232. 20232. 20232. ... 20232. 20232. 20232.]\n",
      " [20232. 20232. 20232. ... 20232. 20232. 20232.]\n",
      " [20232. 20232. 20232. ... 20232. 20232. 20232.]]\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "\n",
    "original_collections_to_dsk = client.collections_to_dsk\n",
    "\n",
    "client.collections_to_dsk = modified_collections_to_dsk\n",
    "\n",
    "dask_array = da.ones((1000, 1000), chunks=(100,100)) + 20231\n",
    "\n",
    "# Use client.compute to compute the result\n",
    "future = client.compute(dask_array)\n",
    "result = future.result()\n",
    "\n",
    "client.shutdown()\n",
    "\n",
    "print(\"------------\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.3: Change graph based on hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from dask.distributed import Client\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_collections_to_dsk(collections, *args, **kwargs):\n",
    "    c = datetime.now()\n",
    "    print(\"Inside myFunc. Time = \" + c.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    print(\"Functions already processed\")\n",
    "\n",
    "    global existing_hashes\n",
    "\n",
    "    print(\"Dask Graph Tasks\")\n",
    "    for collection in collections:\n",
    "        graph = collection.dask\n",
    "\n",
    "        print(f\"Task Name: {collection.name}\")\n",
    "        print(\"Dask Graph\")\n",
    "        print(graph)\n",
    "        total_graph_description = []\n",
    "        for i, (layer_name, layer) in enumerate(graph.layers.items()):\n",
    "            short_layer_name = layer_name[:layer_name.rfind('-')]\n",
    "            task_key, task = next(iter(layer.items()))\n",
    "            function = task[0]\n",
    "            task_args = task[1:]\n",
    "            if i == 0:\n",
    "                layer_args = task_args[0]\n",
    "            else:\n",
    "                layer_args = task_args[1]\n",
    "    \n",
    "            layer_description = {\"function:\": short_layer_name,\n",
    "                                 \"layer_length\": len(layer.items()),\n",
    "                                 \"layer_args\": layer_args}\n",
    "            total_graph_description.append(layer_description)\n",
    "        print('total_graph_description:')\n",
    "        pprint(total_graph_description)\n",
    "        graph_hash = hashlib.sha3_384(str(total_graph_description).encode()).hexdigest()\n",
    "        print(f'hash: \\033[93m {graph_hash} \\033[0m')\n",
    "        if graph_hash in existing_hashes:\n",
    "            print('\\nGraph found in existing_hashes... replacing graph')\n",
    "            dask_array = da.blockwise(\n",
    "            lambda x, y: multiply(add(x, y), y),\n",
    "            \"i\",\n",
    "            np.array([5, 6, 7]),\n",
    "            \"i\",\n",
    "            np.array([8, 9, 10]),\n",
    "            \"i\",\n",
    "            dtype=np.int64,\n",
    "            )\n",
    "            collection.dask = dask_array.dask\n",
    "            collection.__dask_keys__ = dask_array.__dask_keys__\n",
    "            print('New Dask Graph')\n",
    "            print(collection.dask)\n",
    "    print('----------------------')\n",
    "\n",
    "    return original_collections_to_dsk(collections, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_hashes = ['ec2f2283b6f02591655b83abe2be321d08b8b54983b0d9083da5eda81e837d490c838fdb882d8891161f832d30ff1cd8']\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def multiply(x, y):\n",
    "    return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside myFunc. Time = 20:57:24\n",
      "Functions already processed\n",
      "Dask Graph Tasks\n",
      "Task Name: add-34435cc4a153f02b208c5e30ee811e3d\n",
      "Dask Graph\n",
      "HighLevelGraph with 2 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7fc0b64ab3a0>\n",
      " 0. ones_like-c53a97836143a00162e3470fef213e1e\n",
      " 1. add-34435cc4a153f02b208c5e30ee811e3d\n",
      "\n",
      "total_graph_description:\n",
      "[{'function:': 'ones_like', 'layer_args': (100, 100), 'layer_length': 100},\n",
      " {'function:': 'add', 'layer_args': 20231, 'layer_length': 100}]\n",
      "hash: \u001b[93m ec2f2283b6f02591655b83abe2be321d08b8b54983b0d9083da5eda81e837d490c838fdb882d8891161f832d30ff1cd8 \u001b[0m\n",
      "\n",
      "Graph found in existing_hashes... replacing graph\n",
      "New Dask Graph\n",
      "HighLevelGraph with 3 layers.\n",
      "<dask.highlevelgraph.HighLevelGraph object at 0x7fc0a6773400>\n",
      " 0. array-71a7394e5f14de7dc72bb0b1b251ce84\n",
      " 1. array-e3622cdb87acdb3eed57eec6c2ef9fb7\n",
      " 2. lambda-83b0b2050c699e9e26592de20f8e625b\n",
      "\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 20:57:25,128 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:25830' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'finalize-236d3b698353fe74f1e05c293988becb'} (stimulus_id='handle-worker-cleanup-1719950245.1277432')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "[104 135 170]\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "\n",
    "original_collections_to_dsk = client.collections_to_dsk\n",
    "\n",
    "client.collections_to_dsk = modified_collections_to_dsk\n",
    "\n",
    "dask_array = da.ones((1000, 1000), chunks=(100,100)) + 20231\n",
    "\n",
    "# Use client.compute to compute the result\n",
    "future = client.compute(dask_array)\n",
    "result = future.result()\n",
    "\n",
    "client.shutdown()\n",
    "\n",
    "print(\"------------\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask_dirac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
