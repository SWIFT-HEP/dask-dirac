{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWIFT-HEP / GridPP Workshop - April 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "The Dirac Client is introduced here.\n",
    "Functionally it works the same as the dask.distributed.Client, but allows for persistent caching.\n",
    "\n",
    "The following cache locations are supported:\n",
    "- `local`: to set the directory use `file:///path/to/cache`\n",
    "\n",
    "Caching options in the works;\n",
    "- `rucio`: to set the directory use `rucio:///path/to/cache`\n",
    "- `dirac`: to set the directory use `dirac:///path/to/cache`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_dirac import DiracClient, DiracCluster\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = DiracCluster(scheduler_options={\"port\": 8786},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scheduler_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DiracClient(cluster, cache_location=\"file:///tmp/dask-cache_05022025\")\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the cache location and show what files are there\n",
    "print(client.cache_location)\n",
    "!ls {client.cache_location[7:]} # remove file:// at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dask DataFrame directly\n",
    "dask_array = da.ones((1e4, 1), chunks=(1)) + 20231\n",
    "#dask_array.visualize()\n",
    "dask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.compute(dask_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the cache location and show what files are there\n",
    "print(client.cache_location)\n",
    "!ls {client.cache_location[7:]} # remove file:// at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU vs CPU\n",
    "\n",
    "This is an LUX-ZEPLIN analysis which builds a model of multi-scatter-single-ionisation (MSSI) events from simulated events.\n",
    "This simulated events are from detector components. \n",
    "In this analysis, the simulations (ROOT files) are read using `uproot`, and then events are looped over, selecting MSSI events.\n",
    "The simulated events here have already gone through a pre-processing so only events classified as single-scatter events are considered.\n",
    "\n",
    "A more detailed step-by-step description of the analysis is as follows:\n",
    "1. Simulations of detector components are stored as ROOT files.\n",
    "2. These files are read using `uproot` into `awkward` arrays.\n",
    "3. A selection is applied to the data to select MSSI events.\n",
    "4. A normalization is applied to get the expected rate of these events.\n",
    "5. Something about building the model.\n",
    "\n",
    "\n",
    "In addition to the above, this analysis also highlights function decorations with numba for CPU and GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from dask.distributed import LocalCluster, Client, progress\n",
    "import glob\n",
    "import pandas as pd\n",
    "import uproot as up\n",
    "import numba\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def evaluate_poly(coeffs, x):\n",
    "    result = 0.0\n",
    "    for c in coeffs:\n",
    "        result = result * x + c\n",
    "    return result\n",
    "\n",
    "@numba.njit\n",
    "def loop_over_events(ss, mc):\n",
    "    is_mssi = np.zeros(len(ss['ss.correctedS1Area_phd']))\n",
    "    is_FV_mssi = np.zeros(len(ss['ss.correctedS1Area_phd']))\n",
    "    is_FV_ROI_mssi = np.zeros(len(ss['ss.correctedS1Area_phd']))\n",
    "    is_FV_ss = np.zeros(len(ss['ss.correctedS1Area_phd']))\n",
    "    is_FV_ROI_ss = np.zeros(len(ss['ss.correctedS1Area_phd']))\n",
    "\n",
    "\n",
    "    wall_poly_coeffs = np.array([-8.14589334e-14, 2.09181587e-10, -2.06758029e-07,\n",
    "                                  1.01366014e-04, -2.69048354e-02, 7.24276394e+01])\n",
    "\n",
    "    for i in range(len(is_mssi)):\n",
    "        nS1 = 0\n",
    "        nS2 = 0\n",
    "        r = np.sqrt(ss['ss.x_cm'][i] ** 2 + ss['ss.y_cm'][i] ** 2)\n",
    "        drift_time = ss['ss.driftTime_ns'][i] / 1000.\n",
    "        boundary_r = evaluate_poly(wall_poly_coeffs, drift_time) - 3\n",
    "        for j in range(mc['mcTruthVertices.nRQMCTruthVertices'][i]):\n",
    "            if 'Skin' in str(mc['mcTruthVertices.volumeName'][i][j]) or 'Scint' in str(mc['mcTruthVertices.volumeName'][i][j]):\n",
    "                continue\n",
    "            if mc['mcTruthVertices.detectedS1Photons'][i][j] > 0.:\n",
    "                nS1 += 1\n",
    "            if mc['mcTruthVertices.detectedS2Photons'][i][j] > 0.:\n",
    "                nS2 += 1\n",
    "        if nS1 > nS2:\n",
    "            is_mssi[i] = 1\n",
    "            # Apply FV cut\n",
    "            if r < boundary_r:\n",
    "                if drift_time < 1030.:\n",
    "                    if drift_time > 71.:\n",
    "                        is_FV_mssi[i] = 1\n",
    "                        # Apply ROI\n",
    "                        if ss['ss.correctedS1Area_phd'][i] < 600:\n",
    "                            if ss['ss.correctedS1Area_phd'][i] > 3:\n",
    "                                if np.log10(ss['ss.correctedS2Area_phd'][i]) < 4.5:\n",
    "                                    if ss['ss.s2Area_phd'][i] > 14.5 * 44.5:\n",
    "                                        is_FV_ROI_mssi[i] = 1\n",
    "        # single scatter rate\n",
    "        if r < boundary_r:\n",
    "            if drift_time < 1030.:\n",
    "                if drift_time > 71.:\n",
    "                    is_FV_ss[i] = 1\n",
    "                    # Apply ROI\n",
    "                    if ss['ss.correctedS1Area_phd'][i] < 600:\n",
    "                        if ss['ss.correctedS1Area_phd'][i] > 3:\n",
    "                            if np.log10(ss['ss.correctedS2Area_phd'][i]) < 4.5:\n",
    "                                if ss['ss.s2Area_phd'][i] > 14.5 * 44.5:\n",
    "                                    is_FV_ROI_ss[i] = 1\n",
    "\n",
    "    return is_mssi, is_FV_mssi, is_FV_ROI_mssi, is_FV_ss, is_FV_ROI_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file):\n",
    "    # Read the file\n",
    "    branches = ['ss.correctedS1Area_phd', 'ss.correctedS2Area_phd', 'ss.s1Area_phd', 'ss.s2Area_phd', 'ss.x_cm', 'ss.y_cm', 'ss.driftTime_ns']\n",
    "    mcBranches = ['mcTruthVertices.nRQMCTruthVertices', 'mcTruthVertices.volumeName', 'mcTruthVertices.detectedS1Photons', 'mcTruthVertices.detectedS2Photons', 'mcTruthEvent.eventWeight']\n",
    "\n",
    "    tfile = up.open(file)\n",
    "    try: # this is to account for empty files where the simulation was empty\n",
    "        t = tfile['Scatters']\n",
    "        mct = tfile['RQMCTruth']\n",
    "\n",
    "        ss = t.arrays(branches)\n",
    "        mc = mct.arrays(mcBranches)\n",
    "\n",
    "        # Now calculate the number of MSSI events\n",
    "        is_mssi, is_FV_mssi, is_FV_ROI_mssi, is_FV_ss, is_FV_ROI_ss = loop_over_events(ss, mc)\n",
    "        eventWeight = mc['mcTruthEvent.eventWeight'][0]\n",
    "\n",
    "        f_name = file.split('/SS_skim_')[1][:-5] # remove .root from the end of the file name\n",
    "\n",
    "        return f_name, len(ss['ss.s1Area_phd']),  sum(is_FV_ss), sum(is_FV_ROI_ss), sum(is_mssi), sum(is_FV_mssi), sum(is_FV_ROI_mssi), eventWeight\n",
    "    except:\n",
    "        return file, 0, 0, 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=2)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the files to be used. \n",
    "In this example, the files are stored locally under `/shared/scratch/ak18773/lz/mssi/`. \n",
    "Each file is a ROOT file containing the output of an `LZLAMA` simulation (the `NEST` handler); more details can be found in [arvix:2001.09363](https://arxiv.org/abs/2001.09363)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"/shared/scratch/ak18773/lz/mssi/*.root\")\n",
    "print(f'N. files to process: {len(files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_results = [dask.delayed(process_file)(file) for file in files]\n",
    "futures = client.compute(delayed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor the progress\n",
    "progress(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once complete, retrieve the results\n",
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=['File', 'nSS', 'nSS FV', 'nSS FV ROI', 'nMSSI', 'nMSSI FV', 'eventWeight'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing\n",
    "Now that we have the fraction of events in each region, we can calculate the rates using the known `decays/day`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = {\n",
    "    \"Co60_CalibrationSourceTubes\": 4690.57902,\n",
    "    \"Co60_DomePMTs\": 3885.410702,\n",
    "    \"K40_BottomTruss\": 28927.99798,\n",
    "    \"K40_DomePMTs\": 88935.50817,\n",
    "    \"Th232-early_BottomTPCPMTBodies\": 38003.65201,\n",
    "    \"Th232-late_BottomTPCPMTBases\": 20626.61384,\n",
    "    \"Th232-late_BottomTPCPMTBodies\": 51716.2229,\n",
    "    \"Th232-late_ForwardFieldResistors\": 77545.76613,\n",
    "    \"Th232-late_HVInnerCone\": 363483.6619,\n",
    "    \"U238-late_AnodeGridWires\": 4316.423461\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple weights by rates and the number of events to get events per day that are SS, MSSI, etc..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask_dirac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
